# ğŸ§  MCP Math Assistant

### **LangGraph + LangChain + MCP + Streamlit**

> A complete, endâ€‘toâ€‘end example showing **how a UI client (Streamlit) talks to an MCP Tool Server using LangChain and LangGraph**, letting an LLM *decide intelligently* when to call tools.

---

## âœ¨ What This Project Does

This project demonstrates a **toolâ€‘aware AI assistant** with a clean separation of concerns:

* ğŸ–¥ï¸ **Streamlit UI** â†’ acts as the **MCP Client**
* ğŸ§° **FastMCP Server** â†’ exposes math functions as tools
* ğŸ§  **LangGraph** â†’ controls agent flow and looping
* ğŸ¤– **LLM (via LangChain)** â†’ decides *when* to call tools

**Result**

> Ask: *â€œWhat is the square root of 81?â€*
> The LLM **calls the MCP tool**, gets `9`, and replies correctly.

---

## ğŸ§± System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit UI    â”‚  â† MCP Client
â”‚  (User Input)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LangGraph Engine     â”‚
â”‚   (Control Flow)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM (LangChain)      â”‚
â”‚   Tool Decision Maker  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ tool call
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Tool Server      â”‚
â”‚   (FastMCP â€“ Math)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ mcp_client.py   # Streamlit UI + MCP client + LangGraph logic
â”œâ”€â”€ mcp_server.py   # FastMCP tool server (math operations)
â”œâ”€â”€ .env            # OpenAI API key
â”œâ”€â”€ README.md
```

---

## ğŸ”¹ MCP Tool Server (`mcp_server.py`)

### ğŸ¯ Purpose

This file **exposes Python functions as MCP tools** using **FastMCP**.

These tools can be consumed by:

* LangChain agents
* LangGraph workflows
* Claude Desktop
* Any MCPâ€‘compatible client

---

### ğŸ§° Tools Exposed

| Tool Name     | Description                   |
| ------------- | ----------------------------- |
| `add`         | Add two integers              |
| `multiply`    | Multiply two integers         |
| `divide`      | Safe division with validation |
| `square_root` | Square root with checks       |
| `factorial`   | Factorial with input checks   |

---

### âš™ï¸ How FastMCP Works

```python
mcp = FastMCP("Math")

@mcp.tool()
def add(a: int, b: int) -> int:
    return a + b
```

FastMCP automatically:

* Generates tool schemas
* Handles validation
* Exposes HTTP endpoints

---

### â–¶ï¸ Run MCP Server

```bash
python mcp_server.py
```

Server runs at:

```
http://127.0.0.1:8000/mcp
```

Transport used:

```
streamable_http
```

---

## ğŸ”¹ MCP Client + Agent (`mcp_client.py`)

### ğŸ¯ Purpose

This file is the **brain of the system**. It:

* Accepts user input from Streamlit
* Sends queries to the LLM
* Lets the LLM decide whether tools are needed
* Executes MCP tools if required
* Returns the final answer

---

## ğŸ§  Core Components

### 1ï¸âƒ£ MCP Client Configuration

```python
client = MultiServerMCPClient({
    "math": {
        "transport": "streamable_http",
        "url": "http://127.0.0.1:8000/mcp"
    }
})
```

This tells LangChain:

* Where the MCP server lives
* How to communicate with it

---

### 2ï¸âƒ£ Tool Discovery (Dynamic)

```python
tools = await client.get_tools()
```

âœ” No hardcoding
âœ” Auto schema loading
âœ” Plugâ€‘andâ€‘play tools

---

### 3ï¸âƒ£ LLM + Tool Binding

```python
model_with_tools = model.bind_tools(tools)
```

Now the LLM can:

* Answer directly **or**
* Call MCP tools when needed

---

### 4ï¸âƒ£ LangGraph Control Flow

#### ğŸ§© Graph Nodes

| Node Name    | Responsibility     |
| ------------ | ------------------ |
| `call_model` | Calls the LLM      |
| `tools`      | Executes MCP tools |

#### ğŸ”€ Decision Logic

```python
def should_continue(state):
    if last_message.tool_calls:
        return "tools"
    return END
```

âœ¨ This is the magic:

* Tool requested â†’ execute tools
* No tool â†’ finish

---

### 5ï¸âƒ£ Execution Loop

```
START â†’ call_model
          â†“
        tools (if needed)
          â†“
      call_model
          â†“
         END
```

LangGraph automatically loops until **no more tool calls** remain.

---

## ğŸ–¥ï¸ Streamlit UI

Minimal, clean UI:

* Text input for math questions
* Async LangGraph execution
* Final answer rendered cleanly

```python
st.text_input("Ask me something mathâ€‘related:")
```

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Install Dependencies

```bash
pip install streamlit langchain langgraph mcp langchain-mcp-adapters python-dotenv
```

---

### 2ï¸âƒ£ Set Environment Variable

Create a `.env` file:

```
OPENAI_API_KEY=your_api_key_here
```

---

### 3ï¸âƒ£ Start MCP Server

```bash
python mcp_server.py
```

---

### 4ï¸âƒ£ Start Streamlit App

```bash
streamlit run mcp_client.py
```

---

## ğŸ§ª Example Queries

| User Query              | Action Taken           |
| ----------------------- | ---------------------- |
| `What is 5 + 7?`        | Calls `add` tool       |
| `Factorial of 6`        | Calls `factorial` tool |
| `Explain prime numbers` | LLM answers directly   |
| `Square root of -9`     | Tool error surfaced    |

---

## ğŸ§  Key Concepts Demonstrated

* MCP as a **tool protocol**
* LangGraph for **deterministic agent control**
* LLMâ€‘driven tool selection
* Tool execution loops
* UI and agent separation

---

## ğŸš€ Why This Architecture Matters

This pattern scales beautifully to:

* Multiâ€‘agent systems
* A2A protocols
* Claude Desktop tools
* Production AI assistants

You now have:

âœ” Clean separation of concerns
âœ” Replaceable LLMs
âœ” Pluggable tools
âœ” Deterministic agent flow

---

### â­ If you understand this project, you understand modern agentic AI.
